import pandas as pd     #library used for data analysis
import numpy as np      #library used for scientific computation
#import matplotlib.pyplot as plt - provides matlab like plotting framework
from sklearn import metrics        #sklearn is a machine learning library with various algorithms, metrics measure performance
from sklearn.metrics import accuracy_score      #measures accuracy
from sklearn.model_selection import train_test_split        #this function splits data into training and testing datasets
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB      #the different naive bayes algorithm

mydata= pd.read_csv("C:\\xampp\\htdocs\\dataset.csv", low_memory=False)     #importing the dataset. low_memory parameter is set as there are different datatypes
print(mydata)

X=mydata.iloc[:, 0:215].values      #features of the dataset are stored in X, cols 0 to 215
y=mydata.iloc[:, -1].values         #labels (the last col) is stored in variable y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)  #test size gives the percentage of dataset                                                                                              to be the test data, random_state gives a                                                                                              specific value each time is run, if set                                                                                                to a particular number
print(mydata.shape)     #gives the number of rows and cols
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#Multi-variate Bernoulli Naive Bayes The binomial model is useful if your feature vectors are binary (i.e., 0s and 1s). One application would be text classification with a bag of words model where the 0s 1s are "word occurs in the document" and "word does not occur in the document"

#Multinomial Naive Bayes The multinomial naive Bayes model is typically used for discrete counts. E.g., if we have a text classification problem, we can take the idea of bernoulli trials one step further and instead of "word occurs in the document" we have "count how often word occurs in the document", you can think of it as "number of times outcome number x_i is observed over the n trials"

#Gaussian Naive Bayes Here, we assume that the features follow a normal distribution. Instead of discrete counts, we have continuous features (e.g., the popular Iris dataset where the features are sepal width, petal width, sepal length, petal length).

BernNB = BernoulliNB(binarize=0.1)     
BernNB.fit(X_train, y_train)
print(BernNB)       #alpha used for smoothing; binarize used to specify that features have binary values; fit_prior learns class prior                      probabilities and class_prior adjusts the class priors according to data
y_expect=y_test
y_pred=BernNB.predict(X_test)   #performs classification on test data
print (accuracy_score(y_expect, y_pred))

MultiNB = MultinomialNB(alpha=0.2)   #complementnb is a modified version of multinb 
MultiNB.fit(X_train, y_train)
print(MultiNB)
y_pred=MultiNB.predict(X_test)
print (accuracy_score(y_expect, y_pred))

GausNB = GaussianNB()
GausNB.fit(X_train, y_train)
print(GausNB)   #portion of the largest variance of all features that is added to variances for calculation stability
y_pred=GausNB.predict(X_test)
print (accuracy_score(y_expect, y_pred))
